# -*- coding: utf-8 -*-
"""NewFunctioning _RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11pZKDXpKEkAokVi2Wd-pKsv25McikWgr

Before you start:
1. If you have a RAG Sample:
Use this and rename it, use this as a rag sample
2. If you don't have your own rag that you want to use, use the RAG below:
**Name of the File:**chunks.txt

**Contents of the RAG File:**

Test Case 1: Valid Login
def test_valid_login(driver):
    driver.get("https://mail.google.com")
    driver.find_element_by_id("identifierId").send_keys("valid_username")
    driver.find_element_by_id("password").send_keys("valid_password")
    driver.find_element_by_id("signIn").click()
    assert driver.title == "Gmail"

Test Case 2: Invalid Password
def test_invalid_password(driver):
    driver.get("(link unavailable)")
    driver.find_element_by_id("identifierId").send_keys("valid_username")
    driver.find_element_by_id("password").send_keys("invalid_password")
    driver.find_element_by_id("signIn").click()
    assert driver.find_element_by_id("errormsg").text == "Wrong password. Try again or click the Forgot password to reset it."

Test Case 3: Empty Fields
def test_empty_fields(driver):
    driver.get("(link unavailable)")
    driver.find_element_by_id("signIn").click()
    assert driver.find_element_by_id("errormsg").text == "Please enter your email address and password to log in."

Test Case 4: Account Lockout
def test_account_lockout(driver):
    driver.get("(link unavailable)")
    for i in range(5):        driver.find_element_by_id("identifierId").send_keys("invalid_username")        driver.find_element_by_id("password").send_keys("invalid_password")
        driver.find_element_by_id("signIn").click()
    assert driver.find_element_by_id("errormsg").text == "Account locked due to multiple incorrect login attempts."
    
Test Case 5: Forgot Password
def test_forgot_password(driver):
    driver.get("(link unavailable)")
    driver.find_element_by_id("forgotPassword").click()
    driver.find_element_by_id("email").send_keys("valid_username")
    driver.find_element_by_id("submit").click()
    assert driver.find_element_by_id("success msg").text == "Password reset email sent to your registered email address."
"""

!pip install datasets

"""Our file is in .txt so we are converting it to .json and storing it in chunks"""

import json

def split_into_word_chunks(text, words_per_chunk):
    """
    Splits the given text into chunks of approximately `words_per_chunk` words each.

    Args:
    - text (str): The input text to be chunked.
    - words_per_chunk (int): The number of words per chunk.

    Returns:
    - list of str: A list containing the text chunks.
    """
    words = text.split()
    return [' '.join(words[i:i + words_per_chunk]) for i in range(0, len(words), words_per_chunk)]

# Load and process the file
with open('chunks.txt', 'r') as file:
    content = file.read()

# Define the number of words per chunk
words_per_chunk = 100  # Adjust as needed

# Split content into word-sized chunks
chunks = split_into_word_chunks(content, words_per_chunk)

# Create a list of dictionaries for the chunks
data = [{"test_case": chunk} for chunk in chunks]

# Save the chunks into a JSON file
with open('chunks.json', 'w') as json_file:
    json.dump(data, json_file, indent=2)

pip install groq

pip install requests

with open('chunks.txt', 'r') as file:
    text = file.read()

import json

def split_into_word_chunks(text, words_per_chunk):
    """
    Splits the given text into chunks of approximately `words_per_chunk` words each.

    Args:
    - text (str): The input text to be chunked.
    - words_per_chunk (int): The number of words per chunk.

    Returns:
    - list of str: A list containing the text chunks.
    """
    words = text.split()
    return [' '.join(words[i:i + words_per_chunk]) for i in range(0, len(words), words_per_chunk)]

# Load and process the file
with open('chunks.txt', 'r') as file:
    content = file.read()

# Define the number of words per chunk
words_per_chunk = 100  # Adjust as needed

# Split content into word-sized chunks
chunks = split_into_word_chunks(content, words_per_chunk)

# Create a list of dictionaries for the chunks
data = [{"test_case": chunk} for chunk in chunks]

# Save the chunks into a JSON file
with open('chunks.json', 'w') as json_file:
    json.dump(data, json_file, indent=2)

import json

def split_into_word_chunks(text, words_per_chunk):
    """
    Splits the given text into chunks of approximately `words_per_chunk` words each.

    Args:
    - text (str): The input text to be chunked.
    - words_per_chunk (int): The number of words per chunk.

    Returns:
    - list of str: A list containing the text chunks.
    """
    words = text.split()
    return [' '.join(words[i:i + words_per_chunk]) for i in range(0, len(words), words_per_chunk)]

# Load and process the file
with open('chunks.txt', 'r') as file:
    content = file.read()

# Define the number of words per chunk
words_per_chunk = 100  # Adjust as needed

# Split content into word-sized chunks
chunks = split_into_word_chunks(content, words_per_chunk)

# Create a list of dictionaries for the chunks
data = [{"test_case": chunk} for chunk in chunks]

# Save the chunks into a JSON file
with open('chunks.json', 'w') as json_file:
    json.dump(data, json_file, indent=2)

# Load the chunks from the JSON file
with open('chunks.json', 'r') as json_file:
    chunk_data = json.load(json_file)

# Print the chunks
for i, chunk in enumerate(chunk_data):
    print(f"Chunk {i + 1}:\n{chunk['test_case']}\n")

"""Loading the JSON data into a Dataset object we just imported for data manipulation and inspection"""

from datasets import Dataset
dataset = Dataset.from_json('chunks.json') # Loading the JSON file to the dataset object from dataset library
print(dataset[:5])

"""Next lines of code is specific to the RAG file with gmail test cases that I am using. It is basically filling the values in placeholders like : "username" , "password" which we need when our rag is running the text cases."""

def process_chunk(chunk): # new function named process_chunk that will take a single argument called "chunk", which will be used to process each item in the dataset.
    processed_chunk = chunk['test_case'].replace('valid_username', 'user123').replace('valid_password', 'pass123')
    return {"processed_test_case": processed_chunk}
processed_dataset = dataset.map(process_chunk)
print(processed_dataset[:5])

data[0] #just printing the data from first indent

"""Adding an ID Field to Each Entry in the Dataset"""

dataset_with_ids = dataset.map(lambda x, idx: {
    **x,  # making sure it includes all existing fields
    "id": idx  # Adding a unique ID based on the index
}, with_indices=True)

"""Mapping to Format the Data with ID and Metadata"""

formatted_data = dataset_with_ids.map(lambda x: {
    "metadata": {
        "title": x["test_case"],  # Using the 'test_case' for the title
        "content": x["test_case"],  # Using the 'test_case' for the content
    },
    "id": x["id"]  # Keeping the ID in the formatted output
})

"""Dropping the Unnecessary Columns (the original test case field)"""

formatted_data = formatted_data.remove_columns([
    "test_case",
])

"""Printing the Formatted Data"""

print(formatted_data)

"""Printing the first test case"""

first_test_case = formatted_data[0]
print(first_test_case)

"""This is the structure of the data stored"""

import json

# Example data to be written to JSON
data = [
    {
        "id": 1,
        "metadata": {
            "title": "Test Case 1",
            "content": "This is the content of test case 1."
        }
    },
    {
        "id": 2,
        "metadata": {
            "title": "Test Case 2",
            "content": "This is the content of test case 2."
        }
    }
]

# Write the data to a JSON file
with open('test_cases.json', 'w') as json_file:
    json.dump(data, json_file, indent=2)

"""Installing the semantic-router package"""

!pip install semantic-router

"""Since we are using hugging face, installing the HuggingFaceEncoder"""

from semantic_router.encoders import HuggingFaceEncoder

"""Defining the Embedding Model"""

encoder = HuggingFaceEncoder(name="dwzhu/e5-base-4k", max_length=4096)

encoder.device

"""The input to the encoder is a list containing a single string: ["this is a test"], meaning that the encoder will generate embeddings for this one sentence. The encoder tokenizes the text, passes it through the neural network, and obtains a numerical vector (embedding) that represents the input text's semantic meaning. The embeds variable will hold the resulting embeddings. Depending on the implementation of the HuggingFaceEncoder, embeds could be a numpy array, a tensor, or some other data structure containing the numerical embeddings & print(embeds) will print out some changes on  it to make it work for obtaining files from marymount.

"""

embeds = encoder(["this is a test"])
print(embeds)

embeds = encoder(["test case login"])
print(embeds)

""""embeds" is a variable that holds the embeddings generated by the encoder, len(embeds[0]) returns the number of dimensions or components in this embedding vector and this is finally assigned to the dims variable."""

dims = len(embeds[0])
dims

""" installing the "pinecone-client" package"""

!pip install pinecone-client

import os
import getpass
from pinecone import Pinecone

"""Initializing Connection to Pinecone"""

api_key = os.getenv("PINECONE_API_KEY") or getpass.getpass("Enter your Pinecone API key: ")

pc = Pinecone(api_key=api_key)

"""Importing the ServerlessSpec Class"""

from pinecone import ServerlessSpec

spec = ServerlessSpec(
    cloud="aws", region="us-east-1"
)

spec = ServerlessSpec(
    cloud="aws", region="us-east-1"
)

import os
import time
from pinecone import Pinecone, ServerlessSpec

# Fetch API key from environment variable or input manually
api_key = os.getenv("PINECONE_API_KEY") or input("Enter your Pinecone API key: ")

# Create an instance of the Pinecone class
pc = Pinecone(api_key=api_key)

# Define index parameters
index_name = "groq-llama-3-rag"
dims = 768  # Dimension of your embeddings

# List existing indexes
try:
    existing_indexes = pc.list_indexes().names()

    # Check if the index exists
    if index_name not in existing_indexes:
        # Create the index
        pc.create_index(
            name=index_name,
            dimension=dims,
            metric='cosine',
            spec=ServerlessSpec(
                cloud='aws',  # Adjust as necessary
                region='us-east-1'  # Adjust as necessary
            )
        )
        # Wait for the index to be ready
        while not pc.describe_index(index_name).status['ready']:
            time.sleep(1)

    # Connect to the index
    index = pc.Index(index_name)
    time.sleep(1)

    # View index stats
    stats = index.describe_index_stats()
    print(stats)

except Exception as e:
    print(f"An error occurred: {e}")

"""Defining the Variables"""

index_name = "groq-llama-3-rag"
dims = 768

"""Listing Existing Indexes"""

existing_indexes = [index_info["name"] for index_info in pc.list_indexes()]

"""Checking the Index Existence and Creating if Necessary"""

if index_name not in existing_indexes:
    # Create the index if it does not exist
    pc.create_index(
        index_name,
        dimension=dims,
        metric='cosine',
        spec=spec
    )
    # Wait for the index to be initialized
    while not pc.describe_index(index_name).status['ready']:
        time.sleep(1)

"""Connecting to the Index"""

index = pc.Index(index_name)
time.sleep(1)

"""View Index Stats"""

stats = index.describe_index_stats()
print(stats)

from tqdm.auto import tqdm

batch_size = 128

for i in tqdm(range(0, len(data), batch_size)): # Finding the end of the batch
    i_end = min(len(data), i + batch_size) # Creating the batch
    batch = data[i:i_end]
    chunks = [x["test_case"] for x in batch if "test_case" in x]# Create embeddings from test cases
    embeds = encoder(chunks)
    to_upsert = [(x["id"], embed) for x, embed in zip(batch, embeds) if x.get("id") is not None]  # Creating the list for upserting, ensuring IDs are not None
    if to_upsert:     # Ensuring there are items to upsert
        index.upsert(vectors=to_upsert)

batch  # This represents the list of items directly

"""Loading the test cases and Finding the Full Test Case by Query"""

def load_test_cases(file_path):
    with open(file_path, 'r') as file:
        return file.readlines()
def find_full_test_case(test_cases, query):
    full_case = []
    found = False
    for line in test_cases:
        if query.lower() in line.lower():
            found = True
        if found:
            full_case.append(line.strip())
            if line.strip() == "":  # Assuming a blank line indicates end of a case
                break
    return "\n".join(full_case) if full_case else "Test case not found."
# Loading test cases from our file
test_cases = load_test_cases('chunks.txt')
# Defining the query for the invalid login test case
query = "Invalid Password"
result = find_full_test_case(test_cases, query)
print(result)

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer, util
import numpy as np
# Loading the model
model = SentenceTransformer('all-MiniLM-L6-v2')
def load_test_cases(file_path):
    with open(file_path, 'r') as file:
        return file.read().strip().split("\n\n")  # Split by double newline
def embed_test_cases(test_cases):
    return model.encode(test_cases, convert_to_tensor=True)
def find_best_match(test_cases, query):
    query_embedding = model.encode(query, convert_to_tensor=True)
    test_case_embeddings = embed_test_cases(test_cases)
    # Calculating cosine similarities
    cosine_scores = util.pytorch_cos_sim(query_embedding, test_case_embeddings)
    best_match_index = np.argmax(cosine_scores)
    print("Cosine Scores:", cosine_scores)  # Debugging: showing cosine scores
    print("Best Match Index:", best_match_index)  # Debugging: index of best match
    # Adjusting the threshold for better matching
    if cosine_scores[0][best_match_index] > 0.3:  # Lowered threshold(matt said its good if it's 0.8 or more)
        return test_cases[best_match_index]
    else:
        return "Test case not found."
# Loading all test cases from your file
test_cases = load_test_cases('chunks.txt')
# Debugging: printing loaded test cases
print("Loaded Test Cases:")
for case in test_cases:
    print(case[:50])  # Print the first 50 characters of each test case
# Defining the query
query = "find me the testcase of Invalid Password"
# Finding and printing the best matching test case
result = find_best_match(test_cases, query)
print(result)

query = "find me the testcase of forgotten password"
# Finding and printing the best matching test case
result = find_best_match(test_cases, query)
print(result)

!pip install groq

import os
import getpass
# Setting up the API key for Groq
os.environ["GROQ_API_KEY"] = os.getenv("GROQ_API_KEY") or getpass.getpass("Enter your Groq API key: ")
# Initializing the Groq client
from groq import Groq  # Ensuring the library is installed
groq_client = Groq(api_key=os.environ["GROQ_API_KEY"])

def generate(query: str, docs: list[str]):
    # Preparing the system message with context
    system_message = (
        "You are a helpful assistant that answers questions about AI using the "
        "context provided below.\n\n"
        "CONTEXT:\n"
        "\n---\n".join(docs)
    )
    # Creating the message structure for Groq
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": query}
    ]
    # Generating response using Groq
    chat_response = groq_client.chat.completions.create(
        model="llama3-70b-8192",
        messages=messages
    )
    return chat_response.choices[0].message.content

from sentence_transformers import SentenceTransformer, util
import numpy as np

# Load your model
model = SentenceTransformer('all-MiniLM-L6-v2')

def embed_test_cases(docs):
    # Generate embeddings for the test cases
    return model.encode(docs, convert_to_tensor=True)

def find_best_match(test_cases, query_embedding):
    # Calculate cosine scores
    cosine_scores = util.pytorch_cos_sim(query_embedding, test_case_embeddings)

    # Ensure the scores are in the correct shape
    best_match_index = np.argmax(cosine_scores[0])  # Access the first row
    return test_cases[best_match_index] if cosine_scores[0][best_match_index] > 0.3 else "Test case not found."

def generate(query: str, docs: list[str]):
    # Embed the test cases
    global test_case_embeddings
    test_case_embeddings = embed_test_cases(docs)
    # Embed the query
    query_embedding = model.encode(query, convert_to_tensor=True)
    # Find the best matching test case
    full_test_case = find_best_match(docs, query_embedding)
    # Generate the context for the assistant
    context = "\n---\n".join(docs)
    messages = [
        {"role": "system", "content": "You are a helpful assistant that provides relevant test cases from the document."},
        {"role": "user", "content": f"Please retrieve the full test case related to: {query}"}
    ]

    # Generate response
    chat_response = groq_client.chat.completions.create(
        model="llama3-70b-8192",
        messages=messages
    )
    # Extract response content
    response_content = chat_response.choices[0].message.content
    # Combine results
    return f"{response_content}\n\nFull Test Case:\n{full_test_case}"

# Usage
out = generate(query="give me test case for Invalid Password", docs=test_cases)
print(out)

# Usage
out = generate(query="give me test case for valid login", docs=test_cases)
print(out)

pc.delete_index(index_name) # dont run it, i mean you can but it will delete the data you just loaded